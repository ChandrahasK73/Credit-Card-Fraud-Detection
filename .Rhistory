colnames(spam_data) = c("type", "text")
spam = data.frame(spam_data)
spam_data$type = factor(spam_data$type)
spam_data_corpus = VCorpus(VectorSource(spam_data$text))
spam_data_corpus
spam_data_new = tm_map(spam_data_corpus, removeWords,stopwords())
#spam_data_new = tm_map(spam_data_corpus, removeWords,stopwords())
library(tm)
# Convert to valid UTF-8 encoding
spam_data_corpus <- tm_map(spam_data_corpus, content_transformer(iconv), to = "UTF-8")
# Remove stopwords
spam_data_corpus <- tm_map(spam_data_corpus, removeWords, stopwords("en"))
# Strip whitespace
spam_data_corpus <- tm_map(spam_data_corpus, stripWhitespace)
# Convert to character (if needed)
spam_data_char <- sapply(spam_data_corpus, as.character)
# Access a specific document (e.g., the first one)
doc <- spam_data_char[[1]]
as.character(spam_data_new[[1]])
#spam_data_new = tm_map(spam_data_corpus, removeWords,stopwords())
library(tm)
# Convert to valid UTF-8 encoding
spam_data_corpus <- tm_map(spam_data_corpus, content_transformer(iconv), to = "UTF-8")
spam_data_new = tm_map(spam_data_corpus, removeWords,stopwords())
# Remove stopwords
spam_data_corpus <- tm_map(spam_data_corpus, removeWords, stopwords("en"))
# Strip whitespace
spam_data_corpus <- tm_map(spam_data_corpus, stripWhitespace)
# Convert to character (if needed)
spam_data_char <- sapply(spam_data_corpus, as.character)
# Access a specific document (e.g., the first one)
doc <- spam_data_char[[1]]
as.character(spam_data_new[[1]])
spam_data_new = tm_map(spam_data_new, stripWhitespace)
as.character(spam_data_new[[1]])
spam_data_new = tm_map(spam_data_new, removePunctuation)
as.character(spam_data_new[[1]])
spam_data_new = tm_map(spam_data_new, content_transformer(tolower))
as.character(spam_data_new[[1]])
sms_data_mat = DocumentTermMatrix(spam_data_new)
sms_data_mat
train_data_dtm <- sms_data_mat[1:4179,]
test_data_dtm <- sms_data_mat[4180:5572,]
train_labels <- spam_data[1:4179,]$type
test_labels <- spam_data[4180:5572,]$type
freq_terms <- findFreqTerms(train_data_dtm,10)
train_data <- train_data_dtm[,freq_terms]
test_data <- test_data_dtm[,freq_terms]
library(e1071)
count_convert = function(x){
x= ifelse(x > 0,"Yes","No")
}
train_smsspam = apply(train_data,MARGIN = 2, count_convert)
test_smsspam = apply(test_data,MARGIN = 2, count_convert)
svm = naiveBayes(train_smsspam, train_labels)
predictions_train <- predict(svm,train_smsspam)
predictions_test <- predict(svm,test_smsspam)
library('caret')
train_acc = confusionMatrix(predictions_train, train_labels)
test_acc = confusionMatrix(predictions_test, test_labels)
cat("Training Accuracy is ",train_acc$overall[1],"\n")
cat("Test Accuracy is ",test_acc$overall[1],"\n")
# Load required libraries
library(rpart)
library(rpart.plot)
# Generate synthetic data
set.seed(123)
x1 <- runif(100, 0, 10)
x2 <- runif(100, 0, 10)
y <- ifelse(x1 + x2 < 10, "Region1", ifelse(x1 + x2 < 15, "Region2", "Region3"))
# Create a dataframe
data <- data.frame(x1, x2, y)
# Train a decision tree
tree <- rpart(y ~ x1 + x2, data = data, method = "class")
# Plot the decision tree
rpart.plot(tree, type = 3, extra = 1, under = TRUE, varlen = 0, faclen = 0, main = "Decision Tree")
# Plot the regions
plot(x1, x2, col = as.integer(factor(y)), pch = 19, main = "Partition of Feature Space")
legend("topright", legend = levels(data$y), col = 1:length(levels(data$y)), pch = 19)
# Load required libraries
# Load required libraries
library(rpart)
library(rpart.plot)
# Generate synthetic data
set.seed(123)
x1 <- runif(100, 0, 10)
x2 <- runif(100, 0, 10)
y <- ifelse(x1 + x2 < 10, "Region1", ifelse(x1 + x2 < 15, "Region2", "Region3"))
# Create a dataframe
data <- data.frame(x1, x2, y)
# Train a decision tree
tree <- rpart(y ~ x1 + x2, data = data, method = "class")
# Plot the decision tree
rpart.plot(tree, type = 3, extra = 1, under = TRUE, varlen = 0, faclen = 0, main = "Decision Tree")
# Plot the regions
plot(x1, x2, col = as.integer(factor(y)), pch = 19, main = "Partition of Feature Space")
if (length(levels(data$y)) > 0) {  # Check if there are levels in y
legend("topright", legend = levels(data$y), col = 1:length(levels(data$y)), pch = 19)
}
#Let us consider the value of p as follows:
p = seq(0, 1, 0.01)
#Calculating the values of gini index, entropy and classification error by applying formulas:
gini_index = p * (1 - p) +  (1-p)*p
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.error = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini_index, entropy, class.error), col = c("green", "brown", "red"))
# Define the sequence of p^m1 values
p_hat_m1 <- seq(0, 1, 0.01)
# Calculate the values of Gini index, classification error, and entropy
gini_index <- p_hat_m1 * (1 - p_hat_m1) + (1 - p_hat_m1) * p_hat_m1
classification_error <- 1 - pmax(p_hat_m1, 1 - p_hat_m1)
entropy <- -(p_hat_m1 * log(p_hat_m1) + (1 - p_hat_m1) * log(1 - p_hat_m1))
# Plotting
plot(p_hat_m1, gini_index, type = "l", col = "red", xlab = expression(hat(p)[m1]), ylab = "Value", main = "Gini Index, Classification Error, and Entropy")
lines(p_hat_m1, classification_error, type = "l", col = "green")
lines(p_hat_m1, entropy, type = "l", col = "blue")
legend("topright", legend = c("Gini Index", "Classification Error", "Entropy"), col = c("red", "green", "blue"), lty = 1)
library(graphics)
# Create a new plot
plot(1, type = "n", xlab = "X1", ylab = "X2", xlim = c(0, 1), ylim = c(0, 1), main = "Partition of Predictor Space")
# Draw the partitions
segments(0, 0.5, 0.5, 0.5, lty = 1)
segments(0.5, 0, 0.5, 0.5, lty = 1)
# Draw the labels
text(0.25, 0.25, "Mean = 0.21", cex = 0.8)
text(0.75, 0.25, "Mean = -1.06", cex = 0.8)
text(0.25, 0.75, "Mean = 2.49", cex = 0.8)
text(0.75, 0.75, "Mean = -1.8", cex = 0.8)
# Add legend
legend("topright", legend = c("X1 < 0.5", "X2 < 0.5"), lty = 1, col = "black", cex = 0.8)
X1=seq(-2,2,0.1)
X2<- 1+3*X1
plot(X1,X2,xlab='X1',ylab='X2',type='l',xlim=c(-2,2),ylim=c(-2,2))
for(i in seq(-2,2,length.out = 25)){
pts=data.frame(rep(i,25),seq(-2,2,length.out = 25))
points(pts,col=ifelse(1+3*pts[,1]-pts[,2]>0,'green','red'))
}
X1=seq(-2,2,0.1)
X2<- 1-X1/2
plot(X1,X2,xlab='X1',ylab='X2',type='l',xlim=c(-2,2),ylim=c(-2,2))
for(i in seq(-2,2,length.out = 25)){
pts=data.frame(rep(i,25),seq(-2,2,length.out = 25))
points(pts,col=ifelse(-2+pts[,1]+pts[,2]*2>0,'blue','pink'))
}
X1=seq(-2,2,0.1)
X2<- 1+3*X1
plot(X1,X2,xlab='X1',ylab='X2',type='l',xlim=c(-2,2),ylim=c(-2,2))
lines(X1,1-1/2*X1)
for(i in seq(-2,2,length.out = 25)){
pts=data.frame(rep(i,25),seq(-2,2,length.out = 25))
points(pts,col=ifelse(1+3*pts[,1]-pts[,2]>0,'green','red'),
pch=ifelse(-2+pts[,1]+pts[,2]*2>0,1,2))
}
X1=seq(-3,10,0.01)
X2<- 2 - sqrt ( 4-(1+X1)^2)
X3<- 2 + sqrt ( 4-(1+X1)^2)
plot(X1,X2,xlab='X1',ylab='X2',type='l',xlim=c(-3,3),ylim=c(0,4.5))
lines(X1,X3)
for(i in seq(-4,4.5,length.out = 25)){
pts=data.frame(rep(i,25),seq(-4,4.5,length.out = 25))
points(pts,col=ifelse((1+pts[,1])^2+(2-pts[,2])^2>4,'blue','red'))
}
X1=seq(-3,10,0.01)
X2<- 2 - sqrt ( 4-(1+X1)^2)
X3<- 2 + sqrt ( 4-(1+X1)^2)
plot(X1,X2,xlab='X1',ylab='X2',type='l',xlim=c(-3,3),ylim=c(0,4.5))
lines(X1,X3)
for(i in seq(-4,4.5,length.out = 25)){
pts=data.frame(rep(i,25),seq(-4,4.5,length.out = 25))
points(pts,col=ifelse((1+pts[,1])^2+(2-pts[,2])^2>4,'blue','red'))
}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.3, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(2.5), c(1), col = c("red"))
#install.packages('tree')
#install.packages('rpart.plot')
library(caret)
library(tree)
library(rpart)
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart.plot)
#Instead of writing code at each stage to find gini index, entropy, it would be easy if we create a function and call them whenever required:
gini <- function(p)
{
#As it is binary decision tree,
gini_index = 2 * p * (1 - p)
return (gini_index)
}
entropy <- function(p)
{
entropy_value = (p * log(p) + (1 - p) * log(1 - p))
return (entropy_value)
}
set.seed(200)
a<-rnorm(n=200,mean=5,sd=2)
b<-rnorm(n=200,mean=-5,sd=2)
df1 <- data.frame(val = a,label=rep("y",200))
df2 <- data.frame(val = b,label=rep("n",200))
df <- rbind(df1,df2)
df$label <- as.factor(df$label)
decision_tree <- rpart(label~val,df,method="class")
rpart.plot(decision_tree)
p=c(0.4, 0, 0.99)
gini_values=sapply(p, gini)
gini_values
entropy_values=sapply(p, entropy)
entropy_values
set.seed(150)
a1<-rnorm(n=150,mean=1,sd=2)
b1<-rnorm(n=150,mean=-1,sd=2)
df3 <- data.frame(val = a1,label=rep("y",150))
df4 <- data.frame(val = b1,label=rep("n",150))
df11 <- rbind(df3,df4)
df11$label <- as.factor(df11$label)
decision_tree1 <- rpart(label~val,df11,method="class")
rpart.plot(decision_tree1)
p1=c(.5,0.22,0.72,0.28,0.53,0.45,0.09,0.23,0.70,0.37,0.59,1.0,0.81)
gini_values1=sapply(p1, gini)
gini_values1
entropy_values1=sapply(p1, entropy)
entropy_values1
new_tree <- prune.rpart(decision_tree1,cp=0.1)
rpart.plot(new_tree)
p2=c(.5,0.22,0.72)
gini_values2=sapply(p2, gini)
gini_values2
entropy_values2=sapply(p2, entropy)
entropy_values2
library(caret)
library(rpart)
library(rpart.plot)
#install.packages("randomForest")
library(randomForest)
winequality.red <- read.csv("C:\\Users\\chand\\Downloads\\DPA all assignments\\DPA all assignments\\assignment -4\\winequality-red.csv", sep=";")
library(readr)
winequality_white <- read.csv("C:\\Users\\chand\\Downloads\\DPA all assignments\\DPA all assignments\\assignment -4\\winequality-white.csv", sep=";")
#View(winequality_white)
str(winequality_white)
library(readr)
winequality_red <- read.csv("C:\\Users\\chand\\Downloads\\DPA all assignments\\DPA all assignments\\assignment -4\\winequality-red.csv", sep=";")
#View(winequality_red)
str(winequality_red)
winequality_white$quality <- as.factor(winequality_white$quality)
inTrain <- createDataPartition(winequality_white$quality, p = 0.8, list = F)
train.white <- winequality_white[inTrain,]
test.white <- winequality_white[-inTrain,]
winequality_red$quality <- as.factor(winequality_red$quality)
inTrainr <- createDataPartition(winequality_red$quality, p = 0.8, list = F)
train.red <- winequality_red[inTrainr,]
test.red <- winequality_red[-inTrainr,]
dectree_train_white <- rpart(quality~., data=train.white )
rpart.plot(dectree_train_white)
dectree_train_red <- rpart(quality~., data=train.red )
rpart.plot(dectree_train_red)
dectree.predict.white <- predict(dectree_train_white, test.white, type = 'class')
confusionMatrix(dectree.predict.white, test.white$quality)
dectree.predict.red <- predict(dectree_train_red, test.red, type = 'class')
confusionMatrix(dectree.predict.red, test.red$quality)
rf_train_white <- train(quality ~ ., data = train.white, method = "rf",preProcess = c("center","scale"))
rf_predict_white <- predict(rf_train_white, test.white)
confusionMatrix(rf_predict_white, test.white$quality)
confusionMatrix(rf_predict_white, test.white$quality)
rf_train_red <- train(quality ~ ., data = train.red, method = "rf",preProcess = c("center", "scale"))
rf_predict_red <- predict(rf_train_red, test.red)
confusionMatrix(rf_predict_red, test.red$quality)
library(NLP)
library(readxl)
library(tm)
library(readr)
spam_data =  read.csv("spam.csv",stringsAsFactors=FALSE)
colnames(spam_data) = c("type", "text")
spam = data.frame(spam_data)
spam_data$type = factor(spam_data$type)
spam_data_corpus = VCorpus(VectorSource(spam_data$text))
spam_data_corpus
#spam_data_new = tm_map(spam_data_corpus, removeWords,stopwords())
library(tm)
# Convert to valid UTF-8 encoding
spam_data_corpus <- tm_map(spam_data_corpus, content_transformer(iconv), to = "UTF-8")
spam_data_new = tm_map(spam_data_corpus, removeWords,stopwords())
# Remove stopwords
spam_data_corpus <- tm_map(spam_data_corpus, removeWords, stopwords("en"))
# Strip whitespace
spam_data_corpus <- tm_map(spam_data_corpus, stripWhitespace)
# Convert to character (if needed)
spam_data_char <- sapply(spam_data_corpus, as.character)
# Access a specific document (e.g., the first one)
doc <- spam_data_char[[1]]
as.character(spam_data_new[[1]])
spam_data_new = tm_map(spam_data_new, stripWhitespace)
as.character(spam_data_new[[1]])
spam_data_new = tm_map(spam_data_new, removePunctuation)
as.character(spam_data_new[[1]])
spam_data_new = tm_map(spam_data_new, content_transformer(tolower))
as.character(spam_data_new[[1]])
sms_data_mat = DocumentTermMatrix(spam_data_new)
sms_data_mat
train_data_dtm <- sms_data_mat[1:4179,]
test_data_dtm <- sms_data_mat[4180:5572,]
train_labels <- spam_data[1:4179,]$type
test_labels <- spam_data[4180:5572,]$type
freq_terms <- findFreqTerms(train_data_dtm,10)
train_data <- train_data_dtm[,freq_terms]
test_data <- test_data_dtm[,freq_terms]
library(e1071)
count_convert = function(x){
x= ifelse(x > 0,"Yes","No")
}
train_smsspam = apply(train_data,MARGIN = 2, count_convert)
test_smsspam = apply(test_data,MARGIN = 2, count_convert)
svm = naiveBayes(train_smsspam, train_labels)
predictions_train <- predict(svm,train_smsspam)
predictions_test <- predict(svm,test_smsspam)
library('caret')
train_acc = confusionMatrix(predictions_train, train_labels)
test_acc = confusionMatrix(predictions_test, test_labels)
cat("Training Accuracy is ",train_acc$overall[1],"\n")
cat("Test Accuracy is ",test_acc$overall[1],"\n")
knitr::include_graphics("a.jpeg")
knitr::include_graphics("b.jpeg")
dendrogram = as.dist(matrix(c(0, 0.3, 0.4, 0.7,
0.3, 0, 0.5, 0.8,
0.4, 0.5, 0.0, 0.45,
0.7, 0.8, 0.45, 0.0), nrow = 4))
plot(hclust(dendrogram, method = "complete"))
plot(hclust(dendrogram, method = "single"))
plot(hclust(dendrogram, method = "complete"), labels = c(2,1,4,3))
knitr::include_graphics("c.jpeg")
x <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
set.seed(1)
l_s <- sample(2, nrow(x), replace = T)
l_s
plot(x[, 1], x[, 2], col = (l_s + 1), pch = 20, cex = 2)
c_1 <- c(mean(x[l_s == 1, 1]), mean(x[l_s == 1, 2]))
c_2 <- c(mean(x[l_s == 2, 1]), mean(x[l_s == 2, 2]))
plot(x[,1], x[,2], col=(l_s + 1), pch = 20, cex = 2)
points(c_1[1], c_1[2], col = 2, pch = 4)
points(c_2[1], c_2[2], col = 3, pch = 4)
l_s <- c(1, 1, 1, 2, 2, 2)
plot(x[, 1], x[, 2], col = (l_s + 1), pch = 20, cex = 2)
points(c_1[1], c_1[2], col = 2, pch = 4)
points(c_2[1], c_2[2], col = 3, pch = 4)
c_1 <- c(mean(x[l_s == 1, 1]), mean(x[l_s == 1, 2]))
c_2 <- c(mean(x[l_s == 2, 1]), mean(x[l_s == 2, 2]))
plot(x[,1], x[,2], col=(l_s + 1), pch = 20, cex = 2)
points(c_1[1], c_1[2], col = 2, pch = 4)
points(c_2[1], c_2[2], col = 3, pch = 4)
plot(x[, 1], x[, 2], col=(l_s + 1), pch = 20, cex = 2)
# Download and read the wine data
data_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
data <- read.csv(data_url, sep = ",")
colnames(data) <- c(
"class", "alcohol", "malic_acid", "ash", "alcalinity", "magnesium", "total_phenols", "flavanoids",
"nonflavanoid_phenols", "proanthocyanins", "color_intensity", "hue", "OD280_OD315", "proline"
)
# Display the top six rows of the data
head(data)
# Print means of predictors
print("Mean")
#check the means of predictors
apply(data[,-1],2,mean)
print("--------------------------------------------------------------------------------------------------------------")
# Print variances of predictors
print("Varaince")
#check the variance of the predictors
apply(data[,-1],2,var)
#using prcomp to perform PCA
output <- prcomp(data[,-1],scale=TRUE)
output$rotation
#biplot
biplot(output,scale=0)
library(corrplot)
M <- cor(data)
corrplot(M)
cor(data$malic_acid,data$hue)
screeplot(output)
summary(output)
#Computing the proportion of variance for each principal component
variance <- output$sdev^2
pve <- variance/sum(variance)
#screenplot
par(mfrow=c(1,2))
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ",ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ",main="Screen Plot-2", ylim=c(0,1), type='b')
#Percentage of variance accounted for by PC1 and PC2
temp<-pve[1:2]*100
temp
sum(temp)
# Load necessary libraries
library("factoextra")
library(tidyverse)
# Load the dataset
data("USArrests")
# Convert the dataset to a dataframe
data <- data.frame(USArrests)
# Print mean of the predictors
print("Mean:")
mean_values <- apply(data, 2, mean)
print(mean_values)
print("-------------------------------------------------------------------------------------------------------")
# Print variance of the predictors
print("Variance:")
variance_values <- apply(data, 2, var)
print(variance_values)
# Scaling the dataset
scaled_data <- scale(data, center = TRUE, scale = TRUE)
# Applying K-Means
get_total_withinss <- function(k)
{
kmeans(scaled_data, centers = k, nstart = 20)$tot.withinss
}
# Values of k from 2 to 10
k_values <- 2:10  # Comment specifying values of k
# Compute total within-cluster sum of square values for k from 2 to 10
wss_values <- map_dbl(k_values, get_total_withinss)
wss_values  # Comment specifying the result
# Using the elbow method to find the optimal K value
plot(k_values, wss_values,
type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K",
ylab = "Total within-clusters sum of squares")  # Comment specifying the plot
# Using another method to determine the optimal number of clusters
fviz_nbclust(scaled_data, kmeans, method = "wss")  # Comment specifying the plot
# Creating plots for the optimal k
optimal_kmeans <- kmeans(scaled_data, centers = 4, nstart = 20)
fviz_cluster(optimal_kmeans, data = scaled_data)  # Comment specifying the plot
# Define the URL for the dataset
URL <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
# Read the dataset from the URL
wine <- read.csv(URL, sep = ";")
# Display the first few rows of the dataset
head(wine)  # Comment specifying the display
# Exclude the 'quality' variable
dataset <- wine[, -12]  # Comment specifying the exclusion of the 'quality' variable
# Print mean of the dataset
print("Mean:")
mean_values <- apply(dataset, 2, mean)
print(mean_values)
print("------------------------------------------------------------------------------------------------------")
# Print variance of the dataset
print("Variance:")
variance_values <- apply(dataset, 2, var)
print(variance_values)  # Comment specifying the display of variance
# Apply scaling to the dataset
n_dataset <- scale(dataset, center = TRUE, scale = TRUE)  # Comment specifying the application of scaling
# Performing hierarchical clustering using complete linkage
hc_complete <- hclust(dist(n_dataset), method = "complete")
# Plotting the dendrogram for complete linkage
plot(hc_complete)  # Comment specifying the plot
# Performing hierarchical clustering using single linkage
hc_single <- hclust(dist(n_dataset), method = "single")
# Plotting the dendrogram for single linkage
plot(hc_single)  # Comment specifying the plot
# For complete linkage, extracting the height of the last merge
tail(hc_complete$height, 1)  # Comment specifying the extraction
# For single linkage, extracting the height of the last merge
tail(hc_single$height, 1)  # Comment specifying the extraction
# Applying cutree method on complete linkage
cut_complete <- cutree(hc_complete, h = 27.73476)
# Counting the number of clusters formed
table(cut_complete)  # Comment specifying the count of clusters
# Plotting the dendrogram for complete linkage
plot(hc_complete)
# Highlighting the clusters formed using cutree
rect.hclust(hc_complete, h = 27.73476, border = 2:6)  # Comment specifying the highlighting
# Adding a horizontal line at the specified height
abline(h = 27.73476, col = 'brown')  # Comment specifying the line
# Applying cutree method on single linkage
cut_single <- cutree(hc_single, h = 14.25323)
# Counting the number of clusters formed
table(cut_single)  # Comment specifying the count of clusters
# Plotting the dendrogram for single linkage
plot(hc_single)
# Highlighting the clusters formed using cutree
rect.hclust(hc_single, h = 14.25323, border = 2:6)  # Comment specifying the highlighting
# Adding a horizontal line at the specified height
abline(h = 14.25323, col = 'brown')  # Comment specifying the line
# Adding cluster assignments to the dataset for complete linkage
dataset$Clusters <- cut_complete
# Extracting unique cluster assignments
unique(dataset$Clusters)  # Comment specifying the extraction
# Grouping the dataset by clusters
dataset_grouped <- dplyr::group_by(dataset, Clusters)
# Calculating summary statistics for each cluster
summary_stats_complete <- dplyr::summarise_each(dataset_grouped, funs(mean))  # Comment specifying the calculation
# Calculating the absolute difference in feature means for complete linkage clusters
abs(summary_stats_complete[2, -1] - summary_stats_complete[1, -1])
# Comment specifying the calculation
# Adding cluster assignments to the dataset for single linkage
dataset$Clusters <- cut_single
# Extracting unique cluster assignments
unique(dataset$Clusters)  # Comment specifying the extraction
# Grouping the dataset by clusters for single linkage
dataset_grouped_single <- dplyr::group_by(dataset, Clusters)
# Calculating summary statistics for each cluster for single linkage
summary_stats_single <- dplyr::summarise_each(dataset_grouped_single, funs(mean))  # Comment specifying the calculation
# Calculating the absolute difference in feature means for single linkage clusters
abs(summary_stats_single[2, -1] - summary_stats_single[1, -1])
# Comment specifying the calculation
